{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d5e52d-01d0-470e-b513-fbf8d5790449",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e13900-4fde-4cde-9e07-ee260242ac29",
   "metadata": {},
   "source": [
    "## What is a RL problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac5f28-f890-4ffb-9b8e-c3fd9d150331",
   "metadata": {},
   "source": [
    " > Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c2bc6-46c5-4b05-a6f8-c44ae195197d",
   "metadata": {},
   "source": [
    "- RL is applicable to a wide range of seemingly different learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e502f01-4268-4d1c-8b25-1d058f366027",
   "metadata": {},
   "source": [
    "**RL terminology:**\n",
    "\n",
    "What is an agent? And an environment? What are exactly these actions the agent can take? And the reward? Why do you say cumulative reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f3381-d5d4-4b07-b835-2b4653a72aee",
   "metadata": {},
   "source": [
    "#### Example: Learning to drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e73eb1-58bd-403a-8669-8e5443c7c39e",
   "metadata": {},
   "source": [
    "- The __agent__ is the driver who wants to get from A to B, comfortably.\n",
    "\n",
    "- The __state__ of the environment the driver observes has lots of things, including the position, speed and acceleration of the car, all other cars, passengers, road conditions or traffic signs. Transforming such a big vector of inputs into an appropriate action is challenging as you can imagine.\n",
    "\n",
    "  \n",
    "- The __actions__ are basically three: the direction of the steering wheel, throttle intensity and break intensity.\n",
    "\n",
    "\n",
    "- The __reward__ after each action is a weighted sum of the different aspects you need to balance when driving. A decrease in distance to point B brings a positive reward, while an increase a negative one. To ensure no collisions, getting too close (or even colliding) with another car, or even a pedestrian should have a very big negative reward. Also, in order to encourage the smooth driving, sharp changes in speed or direction contribute to a negative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0da6a-cffe-446b-86eb-79c34622e232",
   "metadata": {},
   "source": [
    "<img src=\"img/rl.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744051f4-a3ae-40ee-8292-c0d1b564b970",
   "metadata": {},
   "source": [
    "#### The reward hypothesis: the central idea of Reinforcement Learning\n",
    "#### Why is the goal of the agent to maximize the expected return?\n",
    "\n",
    "Because RL is based on the reward hypothesis, which is that all goals can be described as the maximization of the expected return (expected cumulative reward).\n",
    "\n",
    "That’s why in Reinforcement Learning, to have the best behavior, we aim to learn to take actions that maximize the expected cumulative reward.\n",
    "\n",
    "__Markov Property__\n",
    "\n",
    "The RL process is called a Markov Decision Process (MDP).\n",
    "\n",
    "The Markov Property implies that our agent needs only the current state to decide what action to take and not the history of all the states and actions they took before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c7594-f624-4471-9c61-9a5d44776ec5",
   "metadata": {},
   "source": [
    "**Action Space**\n",
    "  \n",
    "The Action space is the set of all possible actions in an environment.\n",
    "\n",
    "The actions can come from a **discrete** or **continuous** space:\n",
    "\n",
    "**Discrete space:** the number of possible actions is finite.\n",
    "\n",
    "**Continuous space:** the number of possible actions is infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035a46c-bcab-4c2a-846a-040b52a5dbb2",
   "metadata": {},
   "source": [
    "Now that we understand how to formulate an RL problem, we need to solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2db633-e99c-496f-a6b8-6825bbf619d2",
   "metadata": {},
   "source": [
    "## Policies and value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d487765-4c0b-4f5e-9f52-a776c715cc47",
   "metadata": {},
   "source": [
    "#### Policies\n",
    "\n",
    "The agent picks the action she thinks is the best based on the current state of the environment. This is the agent’s strategy, commonly referred to as the agent’s policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ec67e-de09-4253-a300-b0fe9c6d2bd3",
   "metadata": {},
   "source": [
    "> A policy is a learned mapping from states to actions.\n",
    "\n",
    "> Solving a reinforcement learning probem means finding the best possible policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79fed95-b594-4d8a-97f6-96568def0659",
   "metadata": {},
   "source": [
    "Policies are either __deterministic__, when they map each state to one action,\n",
    "\n",
    "$$π(state) = action$$\n",
    "\n",
    "\n",
    "or __stochastic__ when they map each state to a probability distribution over all possible actions.\n",
    "\n",
    "$$π(state)=(prob \\, action \\, 1, prob \\, action \\, 2, ... prob \\, action \\, N)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f65396-ed87-4ce4-aca9-dd01ea0b42bd",
   "metadata": {},
   "source": [
    "__There exist several methods to actually compute this optimal policy. These are called policy optimization methods.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e262e3-a002-4c7a-aa32-72e160492689",
   "metadata": {},
   "source": [
    "### Value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52c96a-505d-49fe-9b2b-d5ac92489f5b",
   "metadata": {},
   "source": [
    "Sometimes, depending on the problem, instead of directly trying to find the optimal policy, one can try to find the value function associated with that optimal policy.\n",
    "\n",
    "But, what is a value function? And before that, what does value mean in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605efc76-13f4-4e45-882d-9a63797d0c27",
   "metadata": {},
   "source": [
    "> The value is a number associated with each state s of the environment that estimates how good it is for the agent to be in state s.\n",
    "\n",
    "> It is the cumulative reward the agent collects when starting at state s and choosing actions according to policy π .\n",
    "\n",
    "> A value function is a learned mapping from states to values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8e7ae-4fcd-455d-85f4-d312fcf64452",
   "metadata": {},
   "source": [
    "The value function of a policy is commonly denoted as\n",
    "\n",
    "$$v_{π}(s)\\, = \\text{cumulative reward when the agent starts at state s and follows policy π}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02c904-cc32-4f40-8ab8-bbd2c1f91dbf",
   "metadata": {},
   "source": [
    "Value functions can also map pairs of (action, state) to values. In this case, they are called q-value functions.\n",
    "\n",
    "$$q_{π}(s,a)= \\text {cumulative reward when the agent start at state s takes action a and follows policy π thereafter}$$\n",
    "\n",
    "The optimal value function (or q-value function) satisfies a mathematical equation, called the Bellman equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0a076-7ec7-44c4-a078-37e27a1dfc49",
   "metadata": {},
   "source": [
    "<img src=\"img/belleq.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738da71-4f3b-488b-abd5-5ce6f607a9f4",
   "metadata": {},
   "source": [
    "This equation is useful because it can be transformed into an iterative procedure to find the optimal value function.\n",
    "\n",
    "__Note__: We define a discount rate called gamma. It must be between 0 and 1. Most of the time between 0.95 and 0.99.\n",
    "\n",
    "But, why are value functions useful?\n",
    "\n",
    "Because you can infer an optimal policy from an optimal q-value function.\n",
    "\n",
    "How?\n",
    "\n",
    "The optimal policy is the one where at each state __s__ the agent chooses the action __a__ that maximizes the q-value function.\n",
    "\n",
    "So, you can jump from optimal policies to optimal q-functions, and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a6d853-8f9f-4052-894f-dc0503e751f1",
   "metadata": {},
   "source": [
    "__Two main approaches for solving RL problems__\n",
    "\n",
    "- Policy-Based Methods\n",
    "  \n",
    "    - teach the agent to learn which action to take, given the current state\n",
    " \n",
    "      \n",
    "      \n",
    "- Value-based methods\n",
    "\n",
    "\n",
    "\n",
    "    - teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6981eb-7030-4440-88e1-bfb13fb87d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes: int):\n",
    "    \"\"\"\n",
    "    Pseudo-code of a Reinforcement Learning agent training loop\n",
    "    \"\"\"\n",
    "\n",
    "    env = load_env()\n",
    "\n",
    "    agent = get_rl_agent()\n",
    "\n",
    "    for episode in range(0, n_episodes):\n",
    "\n",
    "        # random start of the environmnet\n",
    "        state = env.reset()\n",
    "\n",
    "        # epsilon is parameter that controls the exploitation-exploration trade-off.\n",
    "        # it is good practice to set a decaying value for epsilon\n",
    "        epsilon = get_epsilon(episode)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                # Explore action space\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Exploit learned values (or policy)\n",
    "                action = agent.get_best_action(state)\n",
    "\n",
    "            # environment transitions to next state and maybe rewards the agent.\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # adjust agent parameters.\n",
    "            agent.update_parameters(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade5ff0-629c-4437-a44e-53f63d4fdd97",
   "metadata": {},
   "source": [
    "__Epsilon__\n",
    "\n",
    "It is a value between 0 and 1, and it represents the probability the agent chooses a random action instead of what she thinks is the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2c6856-cee2-4074-9cc6-0cbf4bc98ebb",
   "metadata": {},
   "source": [
    "\n",
    "This tradeoff between exploring new strategies vs sticking to already known ones is called the __exploration-exploitation__ problem. This is a key ingredient in RL problems and something that distinguishes RL problems from supervised machine learning.\n",
    "\n",
    "__Exploration__ is exploring the environment by trying random actions in order to find more information about the environment.\n",
    "\n",
    "__Exploitation__ is exploiting known information to maximize the reward.\n",
    "\n",
    "\n",
    "Technically speaking, we want the agent to find the global optimum, not a local one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
